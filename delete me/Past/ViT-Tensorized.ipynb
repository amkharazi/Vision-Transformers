{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","collapsed_sections":["lsjI71PgxfdJ","WqAa29qjgX-g"],"authorship_tag":"ABX9TyOUkbaOq1iNMYkxavzAX+sl"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["!pip install einops"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MGaji1_VgI3S","executionInfo":{"status":"ok","timestamp":1702084100150,"user_tz":-210,"elapsed":10568,"user":{"displayName":"Amir Mohammad Kharazi","userId":"18414030253865360797"}},"outputId":"687fd948-0a78-419c-ef78-31caaeee7485"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (0.7.0)\n"]}]},{"cell_type":"markdown","source":["## Testing Code Blocks"],"metadata":{"id":"j8o2V045xZup"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","import torch.backends.cudnn as cudnn\n","import numpy as np\n","\n","import torchvision\n","import torchvision.transforms as transforms\n","\n","import os\n","import argparse\n","import pandas as pd\n","import csv\n","import time"],"metadata":{"id":"rhXG_NYzHqNf","executionInfo":{"status":"ok","timestamp":1702084137380,"user_tz":-210,"elapsed":7494,"user":{"displayName":"Amir Mohammad Kharazi","userId":"18414030253865360797"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["size = 32\n","bs = 512\n","\n","transform_train = transforms.Compose([\n","    transforms.RandomCrop(32, padding=4),\n","    transforms.Resize(size),\n","    transforms.RandomHorizontalFlip(),\n","    transforms.ToTensor(),\n","    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n","])\n","\n","transform_test = transforms.Compose([\n","    transforms.Resize(size),\n","    transforms.ToTensor(),\n","    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n","])\n","\n","trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n","trainloader = torch.utils.data.DataLoader(trainset, batch_size=bs, shuffle=True, num_workers=8)\n","\n","testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n","testloader = torch.utils.data.DataLoader(testset, batch_size=100, shuffle=False, num_workers=8)\n","\n","classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n","\n","\n","def pair(t):\n","    return t if isinstance(t, tuple) else (t, t)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"03Ne96R4zxMJ","executionInfo":{"status":"ok","timestamp":1702084138584,"user_tz":-210,"elapsed":1211,"user":{"displayName":"Amir Mohammad Kharazi","userId":"18414030253865360797"}},"outputId":"36a5982f-623f-45c3-f86c-8ed32619bd17"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Files already downloaded and verified\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n"]},{"output_type":"stream","name":"stdout","text":["Files already downloaded and verified\n"]}]},{"cell_type":"code","source":["from einops import rearrange, repeat\n","from einops.layers.torch import Rearrange\n","\n","# Patch Embedding Old VIT\n","\n","for batch_idx, (inputs, targets) in enumerate(trainloader):\n","    break\n","\n","print(f'input shape is {inputs.shape}')\n","\n","dim = 512\n","\n","channels = 3\n","\n","image_height, image_width = pair(32)\n","patch_height, patch_width = pair(4)\n","\n","num_patches = (image_height // patch_height) * (image_width // patch_width)\n","patch_dim = channels * patch_height * patch_width\n","\n","print(f'number of patches : {num_patches}')\n","print(f'patch dim = {patch_dim}')\n","\n","to_patch_embedding = nn.Sequential(\n","            Rearrange('b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1 = patch_height, p2 = patch_width),\n","            nn.Linear(patch_dim, dim),\n","        )\n","\n","\n","x_emb_old = to_patch_embedding(inputs)\n","print(f'patch embedded shape : {x_emb_old.shape}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZCbaUMfrxUq0","executionInfo":{"status":"ok","timestamp":1702084145087,"user_tz":-210,"elapsed":3191,"user":{"displayName":"Amir Mohammad Kharazi","userId":"18414030253865360797"}},"outputId":"e4185b2f-babb-4f63-9e7c-578d7eda38f3"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["input shape is torch.Size([512, 3, 32, 32])\n","number of patches : 64\n","patch dim = 48\n","patch embedded shape : torch.Size([512, 64, 512])\n"]}]},{"cell_type":"code","source":["# new patch embedding  part 1 X_ij\n","\n","channels = 3\n","\n","image_height, image_width = pair(32)\n","patch_height, patch_width = pair(4)\n","\n","num_patches = (image_height // patch_height) * (image_width // patch_width)\n","patch_dim = channels * patch_height * patch_width\n","\n","print(f'number of patches : {num_patches} = {np.sqrt(num_patches)}x{np.sqrt(num_patches)}')\n","print(f'patch dim = {patch_dim}  = {channels} x {patch_height} x {patch_width}')\n","\n","\n","Rearrange_L1 = Rearrange('b c (h q1) (w q2) -> b q1 q2 h w c', h = patch_height, w = patch_width)\n","x = Rearrange_L1(inputs)\n","print(f'First Rearrange X_ij : {x.shape}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KGbYVBP01lvA","executionInfo":{"status":"ok","timestamp":1702084145582,"user_tz":-210,"elapsed":13,"user":{"displayName":"Amir Mohammad Kharazi","userId":"18414030253865360797"}},"outputId":"a22e251e-10a8-4279-cd92-632ccb2edf11"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["number of patches : 64 = 8.0x8.0\n","patch dim = 48  = 3 x 4 x 4\n","First Rearrange X_ij : torch.Size([512, 8, 8, 4, 4, 3])\n"]}]},{"cell_type":"code","source":["# https://pytorch.org/docs/stable/generated/torch.einsum.html\n","\n","w = nn.Parameter(torch.randn(4, 32), requires_grad=True)\n","\n","temp = torch.einsum(\"abcdef,dg,eh->abcghf\", (x, w, w))\n","print(f'Shape after n-mode product is : {temp.shape}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Odt2YGzL7gNf","executionInfo":{"status":"ok","timestamp":1702084149052,"user_tz":-210,"elapsed":425,"user":{"displayName":"Amir Mohammad Kharazi","userId":"18414030253865360797"}},"outputId":"75128d3c-9d42-4ef3-ba6f-c0153d7d43ce"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Shape after n-mode product is : torch.Size([512, 8, 8, 32, 32, 3])\n"]}]},{"cell_type":"code","source":["# new patch embedding  part 2 X_ij - bar\n","\n","# from einops import rearrange, repeat\n","# from einops.layers.torch import Rearrange\n","\n","# for batch_idx, (inputs, targets) in enumerate(trainloader):\n","#     break\n","\n","print(f'input shape is {inputs.shape}')\n","\n","dim = 32\n","\n","channels = 3\n","\n","image_height, image_width = pair(32)\n","patch_height, patch_width = pair(4)\n","\n","num_patches = (image_height // patch_height) * (image_width // patch_width)\n","patch_dim = channels * patch_height * patch_width\n","\n","print(f'number of patches : {num_patches} = {np.sqrt(num_patches)}x{np.sqrt(num_patches)}')\n","print(f'patch dim = {patch_dim}  = {channels} x {patch_height} x {patch_width}')\n","\n","\n","Rearrange_L1 = Rearrange('b c (h q1) (w q2) -> b q1 q2 h w c', h = patch_height, w = patch_width)\n","x = Rearrange_L1(inputs)\n","print(f'First Rearrange X_ij : {x.shape}')\n","\n","W1 =  nn.Parameter(torch.randn(patch_height, dim), requires_grad=True)\n","W2 =  nn.Parameter(torch.randn(patch_height, dim), requires_grad=True)\n","W3 =  nn.Parameter(torch.randn(channels, 1), requires_grad=True)\n","\n","xbar_ = torch.einsum(\"abcdef,dg,eh, fi->abcghi\", (x, W1, W2, W3))\n","print(f'Shape after n-mode product is : {xbar_.shape}')\n","\n","xbar = xbar_.squeeze()\n","print(f'xbar squeezed size is : {xbar.shape}')\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8nfoudAP_Azu","executionInfo":{"status":"ok","timestamp":1702084151239,"user_tz":-210,"elapsed":436,"user":{"displayName":"Amir Mohammad Kharazi","userId":"18414030253865360797"}},"outputId":"91e5f9ab-5f8e-4e84-b681-a808ac714b21"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["input shape is torch.Size([512, 3, 32, 32])\n","number of patches : 64 = 8.0x8.0\n","patch dim = 48  = 3 x 4 x 4\n","First Rearrange X_ij : torch.Size([512, 8, 8, 4, 4, 3])\n","Shape after n-mode product is : torch.Size([512, 8, 8, 32, 32, 1])\n","xbar squeezed size is : torch.Size([512, 8, 8, 32, 32])\n"]}]},{"cell_type":"code","source":["# Calculate Q K V Old Method\n","\n","# x_emb_old\n","# dim = 512\n","# inner_dim = dim_head *  heads = 512 * 8\n","dim = 512\n","inner_dim = 512*8\n","\n","to_qkv = nn.Linear(dim, inner_dim * 3, bias = False)\n","qkv = to_qkv(x_emb_old)\n","\n","print(f'qkv shape is  : {qkv.shape} = ({bs}, {num_patches}, 512*8*3)')\n","\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"it8CYkiCAVO8","executionInfo":{"status":"ok","timestamp":1702084161773,"user_tz":-210,"elapsed":5918,"user":{"displayName":"Amir Mohammad Kharazi","userId":"18414030253865360797"}},"outputId":"1807d010-3da0-495e-cf90-d01b54307d04"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["qkv shape is  : torch.Size([512, 64, 12288]) = (512, 64, 512*8*3)\n"]}]},{"cell_type":"code","source":["# Calculate Q K V new method\n","# xbar squeezed size is : torch.Size([512, 8, 8, 32, 32])\n","\n","\n","W1Q = nn.Parameter(torch.randn(xbar.shape[3], 64), requires_grad=True)\n","W2Q = nn.Parameter(torch.randn(xbar.shape[4], 64), requires_grad=True)\n","\n","W1K = nn.Parameter(torch.randn(xbar.shape[3], 64), requires_grad=True)\n","W2K = nn.Parameter(torch.randn(xbar.shape[4], 64), requires_grad=True)\n","\n","W1V = nn.Parameter(torch.randn(xbar.shape[3], 64), requires_grad=True)\n","W2V = nn.Parameter(torch.randn(xbar.shape[4], 64), requires_grad=True)\n","\n","\n","Q  = torch.einsum(\"abcde,dg,eh->abcgh\", (xbar, W1Q, W2Q))\n","K  = torch.einsum(\"abcde,dg,eh->abcgh\", (xbar, W1K, W2K))\n","V  = torch.einsum(\"abcde,dg,eh->abcgh\", (xbar, W1V, W2V))"],"metadata":{"id":"Q-S89b4dCrzz","executionInfo":{"status":"ok","timestamp":1702084172915,"user_tz":-210,"elapsed":3318,"user":{"displayName":"Amir Mohammad Kharazi","userId":"18414030253865360797"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["print(f'Q shape is : {Q.shape}')\n","print(f'K shape is : {K.shape}')\n","print(f'V shape is : {V.shape}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lQTGnqblF8UZ","executionInfo":{"status":"ok","timestamp":1702084176170,"user_tz":-210,"elapsed":4,"user":{"displayName":"Amir Mohammad Kharazi","userId":"18414030253865360797"}},"outputId":"2cab1378-e7fe-46f9-8667-1b4995efd398"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["Q shape is : torch.Size([512, 8, 8, 64, 64])\n","K shape is : torch.Size([512, 8, 8, 64, 64])\n","V shape is : torch.Size([512, 8, 8, 64, 64])\n"]}]},{"cell_type":"code","source":["# Softmax dimension\n","\n","## TO DO .....\n","\n","attend = nn.Softmax(dim = -1)\n","\n","\n","temp = torch.rand(3,3,4,6)\n","temp2 = attend(temp)\n","\n","torch.sum(temp2[0])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zedgUjUUEY4x","executionInfo":{"status":"ok","timestamp":1702084183702,"user_tz":-210,"elapsed":888,"user":{"displayName":"Amir Mohammad Kharazi","userId":"18414030253865360797"}},"outputId":"087603d6-46be-4a6c-a4d6-025733585613"},"execution_count":12,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor(12.)"]},"metadata":{},"execution_count":12}]},{"cell_type":"code","source":["# Calculate Attention new method :\n","\n","# Q shape is : torch.Size([512, 8, 8, 64, 64])\n","# K shape is : torch.Size([512, 8, 8, 64, 64])\n","# V shape is : torch.Size([512, 8, 8, 64, 64])\n","\n","attend = nn.Softmax(dim = -1)\n","\n","\n","A = torch.einsum(\"abcde,aghed->abcgh\", (Q,K))\n","print(f'A : QK^T shape is : {A.shape}')\n","# QK^T shape is : torch.Size([512, 8, 8, 8, 8])\n","\n","A_ = attend(A)/8  # ??????????????  TO DO\n","\n","\n","Attention_out =  torch.einsum(\"abcde,adefg->abcfg\", (A_, V))\n","\n","print(f'Attention Block Output shape is : {Attention_out.shape}')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"x_0ZFNd9Dwrq","executionInfo":{"status":"ok","timestamp":1702084596683,"user_tz":-210,"elapsed":3536,"user":{"displayName":"Amir Mohammad Kharazi","userId":"18414030253865360797"}},"outputId":"7feffe4d-4ec1-4c87-fa4b-c880cc8b87b3"},"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["A : QK^T shape is : torch.Size([512, 8, 8, 8, 8])\n","Attention Block Output shape is : torch.Size([512, 8, 8, 64, 64])\n"]}]},{"cell_type":"markdown","source":["# The Rest is Old VIT - New VIT is Not Implemented Yet\n","\n","<b>  Needs Further Analysis </b>\n","\n","\n","\n","<font size=\"+200\"><font color= \"red\"> THE REST OF THE CODE IS IRRELEVANT TO THIS PROCESS.\n","NO NEED TO REVIEW THEM\n","\n","</font>\n","</font>"],"metadata":{"id":"WWH1ryXvJfnm"}},{"cell_type":"markdown","source":["# VIT"],"metadata":{"id":"lsjI71PgxfdJ"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"vjF8PHUBtoVK"},"outputs":[],"source":["# https://github.com/lucidrains/vit-pytorch/blob/main/vit_pytorch/vit.py\n","\n","import torch\n","from torch import nn\n","\n","\n","from einops import rearrange, repeat\n","from einops.layers.torch import Rearrange\n","\n","# helpers\n","\n","def pair(t):\n","    return t if isinstance(t, tuple) else (t, t)\n","\n","# classes\n","\n","class PreNorm(nn.Module):\n","    def __init__(self, dim, fn):\n","        super().__init__()\n","        self.norm = nn.LayerNorm(dim)\n","        self.fn = fn\n","    def forward(self, x, **kwargs):\n","        return self.fn(self.norm(x), **kwargs)\n","\n","class FeedForward(nn.Module):\n","    def __init__(self, dim, hidden_dim, dropout = 0.):\n","        super().__init__()\n","        self.net = nn.Sequential(\n","            nn.Linear(dim, hidden_dim),\n","            nn.GELU(),\n","            nn.Dropout(dropout),\n","            nn.Linear(hidden_dim, dim),\n","            nn.Dropout(dropout)\n","        )\n","    def forward(self, x):\n","        return self.net(x)\n","\n","class Attention(nn.Module):\n","    def __init__(self, dim, heads = 8, dim_head = 64, dropout = 0.):\n","        super().__init__()\n","        inner_dim = dim_head *  heads\n","        project_out = not (heads == 1 and dim_head == dim)\n","\n","        self.heads = heads\n","        self.scale = dim_head ** -0.5\n","\n","        self.attend = nn.Softmax(dim = -1)\n","        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias = False)\n","\n","        self.to_out = nn.Sequential(\n","            nn.Linear(inner_dim, dim),\n","            nn.Dropout(dropout)\n","        ) if project_out else nn.Identity()\n","\n","    def forward(self, x):\n","        qkv = self.to_qkv(x).chunk(3, dim = -1)\n","        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h = self.heads), qkv)\n","\n","        dots = torch.matmul(q, k.transpose(-1, -2)) * self.scale\n","\n","        attn = self.attend(dots)\n","\n","        out = torch.matmul(attn, v)\n","        out = rearrange(out, 'b h n d -> b n (h d)')\n","        return self.to_out(out)\n","\n","class Transformer(nn.Module):\n","    def __init__(self, dim, depth, heads, dim_head, mlp_dim, dropout = 0.):\n","        super().__init__()\n","        self.layers = nn.ModuleList([])\n","        for _ in range(depth):\n","            self.layers.append(nn.ModuleList([\n","                PreNorm(dim, Attention(dim, heads = heads, dim_head = dim_head, dropout = dropout)),\n","                PreNorm(dim, FeedForward(dim, mlp_dim, dropout = dropout))\n","            ]))\n","    def forward(self, x):\n","        for attn, ff in self.layers:\n","            x = attn(x) + x\n","            x = ff(x) + x\n","        return x\n","\n","class ViT(nn.Module):\n","    def __init__(self, *, image_size, patch_size, num_classes, dim, depth, heads, mlp_dim, pool = 'cls', channels = 3, dim_head = 64, dropout = 0., emb_dropout = 0.):\n","        super().__init__()\n","        image_height, image_width = pair(image_size)\n","        patch_height, patch_width = pair(patch_size)\n","\n","        assert image_height % patch_height == 0 and image_width % patch_width == 0, 'Image dimensions must be divisible by the patch size.'\n","\n","        num_patches = (image_height // patch_height) * (image_width // patch_width)\n","        patch_dim = channels * patch_height * patch_width\n","        assert pool in {'cls', 'mean'}, 'pool type must be either cls (cls token) or mean (mean pooling)'\n","\n","        self.to_patch_embedding = nn.Sequential(\n","            Rearrange('b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1 = patch_height, p2 = patch_width),\n","            nn.Linear(patch_dim, dim),\n","        )\n","\n","        self.pos_embedding = nn.Parameter(torch.randn(1, num_patches + 1, dim))\n","        self.cls_token = nn.Parameter(torch.randn(1, 1, dim))\n","        self.dropout = nn.Dropout(emb_dropout)\n","\n","        self.transformer = Transformer(dim, depth, heads, dim_head, mlp_dim, dropout)\n","\n","        self.pool = pool\n","        self.to_latent = nn.Identity()\n","\n","        self.mlp_head = nn.Sequential(\n","            nn.LayerNorm(dim),\n","            nn.Linear(dim, num_classes)\n","        )\n","\n","    def forward(self, img):\n","        x = self.to_patch_embedding(img)\n","        b, n, _ = x.shape\n","\n","        cls_tokens = repeat(self.cls_token, '() n d -> b n d', b = b)\n","        x = torch.cat((cls_tokens, x), dim=1)\n","        x += self.pos_embedding[:, :(n + 1)]\n","        x = self.dropout(x)\n","\n","        x = self.transformer(x)\n","\n","        x = x.mean(dim = 1) if self.pool == 'mean' else x[:, 0]\n","\n","        x = self.to_latent(x)\n","        return self.mlp_head(x)"]},{"cell_type":"markdown","source":["# Train On CIFAR10"],"metadata":{"id":"WqAa29qjgX-g"}},{"cell_type":"markdown","source":["## Libraries"],"metadata":{"id":"5gpPTCxykZ4l"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","import torch.backends.cudnn as cudnn\n","import numpy as np\n","\n","import torchvision\n","import torchvision.transforms as transforms\n","\n","import os\n","import argparse\n","import pandas as pd\n","import csv\n","import time\n","\n","# from utils import progress_bar\n","# from randomaug import RandAugment"],"metadata":{"id":"5EUcLI6Qgbiq","executionInfo":{"status":"ok","timestamp":1702078250891,"user_tz":-210,"elapsed":6485,"user":{"displayName":"Amir Mohammad Kharazi","userId":"18414030253865360797"}}},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":["## Hyper parameters"],"metadata":{"id":"Ex9oDanTkbPs"}},{"cell_type":"code","source":["# Hyper parameters\n","lr = 1e-4\n","bs = 512\n","size = 32\n","n_epochs = 20\n","patch = 4\n","dimhead = 512\n","\n","\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","print(device)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YULoCBKUiJ72","executionInfo":{"status":"ok","timestamp":1702078250892,"user_tz":-210,"elapsed":6,"user":{"displayName":"Amir Mohammad Kharazi","userId":"18414030253865360797"}},"outputId":"7ea26761-dc47-4f79-ced9-0a5eb5287822"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["cuda\n"]}]},{"cell_type":"markdown","source":["## Load Dataset - Train/Test Loaders"],"metadata":{"id":"Ab1axAJ9kdhE"}},{"cell_type":"code","source":["transform_train = transforms.Compose([\n","    transforms.RandomCrop(32, padding=4),\n","    transforms.Resize(size),\n","    transforms.RandomHorizontalFlip(),\n","    transforms.ToTensor(),\n","    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n","])\n","\n","transform_test = transforms.Compose([\n","    transforms.Resize(size),\n","    transforms.ToTensor(),\n","    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n","])\n","\n","trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n","trainloader = torch.utils.data.DataLoader(trainset, batch_size=bs, shuffle=True, num_workers=8)\n","\n","testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n","testloader = torch.utils.data.DataLoader(testset, batch_size=100, shuffle=False, num_workers=8)\n","\n","classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UPtHlkOphv5m","executionInfo":{"status":"ok","timestamp":1702078259710,"user_tz":-210,"elapsed":8822,"user":{"displayName":"Amir Mohammad Kharazi","userId":"18414030253865360797"}},"outputId":"61887308-0395-4a28-c29a-97e1df8e9944"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 170498071/170498071 [00:03<00:00, 43007646.68it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Extracting ./data/cifar-10-python.tar.gz to ./data\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n"]},{"output_type":"stream","name":"stdout","text":["Files already downloaded and verified\n"]}]},{"cell_type":"markdown","source":["## Build Model"],"metadata":{"id":"Ua93TxOEki-c"}},{"cell_type":"code","source":["net = ViT(\n","    image_size = size,\n","    patch_size = patch,\n","    num_classes = 10,\n","    dim = dimhead,\n","    depth = 6,\n","    heads = 8,\n","    mlp_dim = 512,\n","    dropout = 0.1,\n","    emb_dropout = 0.1\n",")\n","\n","net"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pGGJSytkkXrO","executionInfo":{"status":"ok","timestamp":1702075228301,"user_tz":-210,"elapsed":5,"user":{"displayName":"Amir Mohammad Kharazi","userId":"18414030253865360797"}},"outputId":"d823f85d-aa9b-46ed-daf8-49e1047eaaf1"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["ViT(\n","  (to_patch_embedding): Sequential(\n","    (0): Rearrange('b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1=4, p2=4)\n","    (1): Linear(in_features=48, out_features=512, bias=True)\n","  )\n","  (dropout): Dropout(p=0.1, inplace=False)\n","  (transformer): Transformer(\n","    (layers): ModuleList(\n","      (0-5): 6 x ModuleList(\n","        (0): PreNorm(\n","          (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","          (fn): Attention(\n","            (attend): Softmax(dim=-1)\n","            (to_qkv): Linear(in_features=512, out_features=1536, bias=False)\n","            (to_out): Sequential(\n","              (0): Linear(in_features=512, out_features=512, bias=True)\n","              (1): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","        )\n","        (1): PreNorm(\n","          (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","          (fn): FeedForward(\n","            (net): Sequential(\n","              (0): Linear(in_features=512, out_features=512, bias=True)\n","              (1): GELU(approximate='none')\n","              (2): Dropout(p=0.1, inplace=False)\n","              (3): Linear(in_features=512, out_features=512, bias=True)\n","              (4): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","        )\n","      )\n","    )\n","  )\n","  (to_latent): Identity()\n","  (mlp_head): Sequential(\n","    (0): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","    (1): Linear(in_features=512, out_features=10, bias=True)\n","  )\n",")"]},"metadata":{},"execution_count":14}]},{"cell_type":"markdown","source":["## Optimizer / Criterion"],"metadata":{"id":"JM0GXjlalVh3"}},{"cell_type":"code","source":["criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(net.parameters(), lr=lr)\n","\n","scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, n_epochs)\n","\n","\n","num_parameters = sum(p.numel() for p in net.parameters() if p.requires_grad)\n","\n","print(f'number of parameters : {num_parameters}')\n","\n","scaler = torch.cuda.amp.GradScaler(enabled=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rP3_MEytlL7b","executionInfo":{"status":"ok","timestamp":1702075440311,"user_tz":-210,"elapsed":8,"user":{"displayName":"Amir Mohammad Kharazi","userId":"18414030253865360797"}},"outputId":"e2013e0e-06fa-4253-8208-eab08aebcb96"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["number of parameters : 9523722\n"]}]},{"cell_type":"markdown","source":["## Def Train/Test"],"metadata":{"id":"cYDHmLlbm1_T"}},{"cell_type":"code","source":["best_acc = 0\n","\n","def train(epoch):\n","    print('\\nEpoch: %d' % epoch)\n","    net.train()\n","    train_loss = 0\n","    correct = 0\n","    total = 0\n","    for batch_idx, (inputs, targets) in enumerate(trainloader):\n","        inputs, targets = inputs.to(device), targets.to(device)\n","        # Train with amp\n","        with torch.cuda.amp.autocast(enabled=True):\n","            outputs = net(inputs)\n","            loss = criterion(outputs, targets)\n","        scaler.scale(loss).backward()\n","        scaler.step(optimizer)\n","        scaler.update()\n","        optimizer.zero_grad()\n","\n","        train_loss += loss.item()\n","        _, predicted = outputs.max(1)\n","        total += targets.size(0)\n","        correct += predicted.eq(targets).sum().item()\n","\n","\n","    acc = 100.*correct/total\n","    # print(f'acc : {acc}')\n","\n","    return train_loss/(batch_idx+1), acc\n","\n","\n","def test(epoch):\n","    global best_acc\n","    net.eval()\n","    test_loss = 0\n","    correct = 0\n","    total = 0\n","    with torch.no_grad():\n","        for batch_idx, (inputs, targets) in enumerate(testloader):\n","            inputs, targets = inputs.to(device), targets.to(device)\n","            outputs = net(inputs)\n","            loss = criterion(outputs, targets)\n","\n","            test_loss += loss.item()\n","            _, predicted = outputs.max(1)\n","            total += targets.size(0)\n","            correct += predicted.eq(targets).sum().item()\n","\n","\n","    acc = 100.*correct/total\n","    # print(f'acc : {acc}')\n","\n","    return test_loss, acc\n"],"metadata":{"id":"RC3MZCO_m4Yr"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Train Model"],"metadata":{"id":"Y8rSXV9tn-AE"}},{"cell_type":"code","source":["train_list_loss = []\n","train_list_acc = []\n","\n","test_list_loss = []\n","test_list_acc = []\n","\n","net.cuda()\n","for epoch in range(0, n_epochs):\n","    start = time.time()\n","    trainloss,train_acc = train(epoch)\n","    val_loss, val_acc = test(epoch)\n","\n","    scheduler.step(epoch-1) # step cosine scheduling\n","\n","    train_list_loss.append(trainloss)\n","    train_list_acc.append(train_acc)\n","\n","    test_list_loss.append(val_loss)\n","    test_list_acc.append(val_acc)\n","\n","    print(f'epoch {epoch}, train loss = {trainloss}, train acc = {train_acc}, test loss = {val_loss}, test acc = {val_acc}, epoch time : {time.time()-start}, lr = {optimizer.param_groups[0][\"lr\"]}')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wcIJ-eZvn-gD","executionInfo":{"status":"ok","timestamp":1702077439950,"user_tz":-210,"elapsed":819220,"user":{"displayName":"Amir Mohammad Kharazi","userId":"18414030253865360797"}},"outputId":"ea817dba-15b2-401a-f283-496d638b418a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Epoch: 0\n","epoch 0, train loss = 7.023257639943337, train acc = 14.928, test loss = 697.6976375579834, test acc = 14.9, epoch time : 42.170133113861084, lr = 9.938441702975689e-05\n","\n","Epoch: 1\n","epoch 1, train loss = 6.552092226184144, train acc = 14.7, test loss = 604.46044921875, test acc = 15.67, epoch time : 39.909051179885864, lr = 0.0001\n","\n","Epoch: 2\n","epoch 2, train loss = 5.50928918682799, train acc = 15.236, test loss = 502.34032249450684, test acc = 16.88, epoch time : 41.441991567611694, lr = 9.938441702975689e-05\n","\n","Epoch: 3\n","epoch 3, train loss = 4.554507211763031, train acc = 17.284, test loss = 437.2686836719513, test acc = 20.75, epoch time : 41.283379793167114, lr = 9.755282581475769e-05\n","\n","Epoch: 4\n","epoch 4, train loss = 4.053258569873109, train acc = 21.468, test loss = 404.33879351615906, test acc = 24.56, epoch time : 40.15654253959656, lr = 9.45503262094184e-05\n","\n","Epoch: 5\n","epoch 5, train loss = 3.7678627821863913, train acc = 23.69, test loss = 376.4938209056854, test acc = 25.8, epoch time : 40.97043180465698, lr = 9.045084971874738e-05\n","\n","Epoch: 6\n","epoch 6, train loss = 3.52104423727308, train acc = 24.886, test loss = 350.19088983535767, test acc = 26.78, epoch time : 41.20109820365906, lr = 8.535533905932738e-05\n","\n","Epoch: 7\n","epoch 7, train loss = 3.2805377585547313, train acc = 25.102, test loss = 325.23206877708435, test acc = 27.35, epoch time : 40.84930610656738, lr = 7.938926261462366e-05\n","\n","Epoch: 8\n","epoch 8, train loss = 3.055329369038952, train acc = 25.426, test loss = 301.6386797428131, test acc = 27.83, epoch time : 40.89665865898132, lr = 7.269952498697734e-05\n","\n","Epoch: 9\n","epoch 9, train loss = 2.8399422874256057, train acc = 25.628, test loss = 279.4894914627075, test acc = 27.98, epoch time : 41.076192140579224, lr = 6.545084971874738e-05\n","\n","Epoch: 10\n","epoch 10, train loss = 2.6357076849256242, train acc = 25.75, test loss = 259.3510694503784, test acc = 28.45, epoch time : 40.70518660545349, lr = 5.782172325201155e-05\n","\n","Epoch: 11\n","epoch 11, train loss = 2.4612300249995016, train acc = 26.214, test loss = 241.901593208313, test acc = 28.8, epoch time : 40.85943269729614, lr = 5e-05\n","\n","Epoch: 12\n","epoch 12, train loss = 2.3137919756830954, train acc = 26.388, test loss = 228.1579189300537, test acc = 29.05, epoch time : 42.16062831878662, lr = 4.217827674798847e-05\n","\n","Epoch: 13\n","epoch 13, train loss = 2.20458076194841, train acc = 26.814, test loss = 218.5265748500824, test acc = 29.25, epoch time : 39.9508638381958, lr = 3.4549150281252636e-05\n","\n","Epoch: 14\n","epoch 14, train loss = 2.137321374854263, train acc = 27.06, test loss = 212.39432382583618, test acc = 29.39, epoch time : 41.03421139717102, lr = 2.7300475013022663e-05\n","\n","Epoch: 15\n","epoch 15, train loss = 2.0942636703958315, train acc = 27.506, test loss = 208.6020942926407, test acc = 29.56, epoch time : 41.22712278366089, lr = 2.061073738537635e-05\n","\n","Epoch: 16\n","epoch 16, train loss = 2.069982134566015, train acc = 27.61, test loss = 206.31113576889038, test acc = 29.74, epoch time : 39.87516212463379, lr = 1.4644660940672627e-05\n","\n","Epoch: 17\n","epoch 17, train loss = 2.0536637549497643, train acc = 27.846, test loss = 204.92634654045105, test acc = 29.9, epoch time : 42.55361485481262, lr = 9.549150281252633e-06\n","\n","Epoch: 18\n","epoch 18, train loss = 2.0462326054670372, train acc = 27.848, test loss = 204.09637796878815, test acc = 30.08, epoch time : 40.11669611930847, lr = 5.449673790581611e-06\n","\n","Epoch: 19\n","epoch 19, train loss = 2.043370492604314, train acc = 28.308, test loss = 203.64966595172882, test acc = 30.09, epoch time : 40.78061532974243, lr = 2.4471741852423237e-06\n"]}]}]}