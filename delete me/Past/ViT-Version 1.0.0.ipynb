{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOEXTbfwuHD/1LATAVOLqO5"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Vision Transformer (ViT) in PyTorch\n","\n","A PyTorch implement of Vision Transformers as described in:\n","\n","'An Image Is Worth 16 x 16 Words: Transformers for Image Recognition at Scale'\n","    - https://arxiv.org/abs/2010.11929\n","\n","`How to train your ViT? Data, Augmentation, and Regularization in Vision Transformers`\n","    - https://arxiv.org/abs/2106.10270\n","\n","`FlexiViT: One Model for All Patch Sizes`\n","    - https://arxiv.org/abs/2212.08013\n","\n","The official jax code is released and available at\n","  * https://github.com/google-research/vision_transformer\n","  * https://github.com/google-research/big_vision\n","\n","Acknowledgments:\n","  * The paper authors for releasing code and weights, thanks!\n","  * I fixed my class token impl based on Phil Wang's https://github.com/lucidrains/vit-pytorch\n","  * Simple transformer style inspired by Andrej Karpathy's https://github.com/karpathy/minGPT\n","  * Bert reference code checks against Huggingface Transformers and Tensorflow Bert\n","\n","Hacked together by / Copyright 2020, Ross Wightman\n"],"metadata":{"id":"sZZd9b2gDkQk"}},{"cell_type":"markdown","source":["# Imports"],"metadata":{"id":"V9LkoF_1Dvvj"}},{"cell_type":"code","execution_count":1,"metadata":{"id":"2Il-RKaeDikU","executionInfo":{"status":"ok","timestamp":1703844844294,"user_tz":-210,"elapsed":5651,"user":{"displayName":"Amir Mohammad Kharazi","userId":"18414030253865360797"}}},"outputs":[],"source":["from collections import OrderedDict\n","from functools import partial\n","from typing import Any, Callable, Dict, Optional, Sequence, Set, Tuple, Type, Union, List\n","try:\n","    from typing import Literal\n","except ImportError:\n","    from typing_extensions import Literal\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.utils.checkpoint\n","from torch.jit import Final"]},{"cell_type":"markdown","source":["# Attention Class"],"metadata":{"id":"U-IIUFnAEOrg"}},{"cell_type":"code","source":["class Attention(nn.Module):\n","\n","    def __init__(\n","            self,\n","            dim: int,\n","            num_heads: int = 8,\n","            qkv_bias: bool = False,\n","            qk_norm: bool = False,\n","            attn_drop: float = 0.,\n","            proj_drop: float = 0.,\n","            norm_layer: nn.Module = nn.LayerNorm,\n","    ) -> None:\n","        super().__init__()\n","        assert dim % num_heads == 0, 'dim should be divisible by num_heads'\n","        self.num_heads = num_heads\n","        self.head_dim = dim // num_heads\n","        self.scale = self.head_dim ** -0.5\n","\n","        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n","        self.q_norm = norm_layer(self.head_dim) if qk_norm else nn.Identity()\n","        self.k_norm = norm_layer(self.head_dim) if qk_norm else nn.Identity()\n","        self.attn_drop = nn.Dropout(attn_drop)\n","        self.proj = nn.Linear(dim, dim)\n","        self.proj_drop = nn.Dropout(proj_drop)\n","\n","    def forward(self, x: torch.Tensor) -> torch.Tensor:\n","        B, N, C = x.shape\n","        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, self.head_dim).permute(2, 0, 3, 1, 4)\n","        q, k, v = qkv.unbind(0)\n","        q, k = self.q_norm(q), self.k_norm(k)\n","\n","        q = q * self.scale\n","        attn = q @ k.transpose(-2, -1) # @ : basically standard symbol for matrix multiplication # -2 -1 two last dims are transposed (1,4) -> (4,1)\n","        attn = attn.softmax(dim=-1)\n","        attn = self.attn_drop(attn)\n","        x = attn @ v\n","\n","        x = x.transpose(1, 2).reshape(B, N, C)\n","        x = self.proj(x)\n","        x = self.proj_drop(x)\n","        return x"],"metadata":{"id":"MaTfWe5rEQqn","executionInfo":{"status":"ok","timestamp":1703845518705,"user_tz":-210,"elapsed":371,"user":{"displayName":"Amir Mohammad Kharazi","userId":"18414030253865360797"}}},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":["## Example\n","\n","in patch embed :\n","\n","- B, C, H, W = x.shape\n","- img_size: Optional[int] = 224,\n","- patch_size: int = 16,\n","- in_chans: int = 3,\n","- embed_dim: int = 768,\n","\n","- x : [ 128, 3, 224, 224 ]\n","\n","after patch embed :\n","\n","- grid_size : ( 224/16 , 224/16 ) = ( 8 , 8 )\n","- num_patches : 8*8 = 64\n","- Convolution Projection  :\n","  - proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size, bias=bias)\n","- shape : [(in - f + 2p)/s]  + 1 =  [ 128, 768 , 14 , 14 ]\n","- 14 is the patch index 224/16 = 14\n","- x.flatten(2).transpose(1, 2) which means flatten to [128,768,14x14] and transposed to [128,14x14,768]\n","- normalize and return"],"metadata":{"id":"jAW6HfsKG2Ud"}},{"cell_type":"code","source":["Temp = Attention(dim = 768, num_heads = 8) # dim should be equal to input dim 768\n","x = torch.rand(128,14*14,768) # x output from embedding is 128, 14*14 , 768\n","print(f'x shape is : {x.shape}')\n","x2 = Temp(x)\n","print(f'x2 shape is : {x2.shape}')\n","# qkv = nn.Linear(768, 768 * 3, bias=qkv_bias)\n","# qkv(x).reshape(B, N, 3, self.num_heads = 8 , self.head_dim = 96).permute(2, 0, 3, 1, 4)\n","# qkv = [3 , 128 , 8 , 14*14 , 96]\n","# q = [128 , 8 , 14*14 , 96]\n","# k = [128 , 8 , 14*14 , 96]\n","# v = [128 , 8 , 14*14 , 96]\n","# q @ k.transpose(-2,-1) -> [128 , 8 , 14*14 , 96] * [128 , 8 , 96 , 14*14] = [128 , 8 , 14*14 , 14*14]\n","# @ v -> [128 , 8 , 14*14 , 14*14] * [128 , 8 , 14*14 , 96] = [128 , 8 , 14*14 , 96]\n","# x .transpose (1,2) : [128 , 8 , 14*14 , 96] --> [128 , 14*14 , 8 , 96]\n","# x.reshape(B, N, C) where B, N, C = x.shape -> 128 , 14*14 , 768\n","# proj = nn.Linear(dim = 768, dim = 768)\n","# out is proj(x)\n","# out shape is [128, 14*14, 768]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9L5F2K0wG3y1","executionInfo":{"status":"ok","timestamp":1703847873771,"user_tz":-210,"elapsed":5327,"user":{"displayName":"Amir Mohammad Kharazi","userId":"18414030253865360797"}},"outputId":"db929954-e224-4fa4-a285-024f76471ffd"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["x shape is : torch.Size([128, 196, 768])\n","x2 shape is : torch.Size([128, 196, 768])\n"]}]}]}