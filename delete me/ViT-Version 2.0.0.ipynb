{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNe7QNEbMwJ4Jhjyb9sFsb5"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Vision Transformers basic Implementation - Version 2.0.0"],"metadata":{"id":"1-TJ_oZLXinG"}},{"cell_type":"markdown","source":["# Import Libraries"],"metadata":{"id":"0qMSCBi7XqAx"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","!pip install einops\n","from einops import rearrange"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"u6lI6azhXsk6","executionInfo":{"status":"ok","timestamp":1703856867920,"user_tz":-210,"elapsed":9440,"user":{"displayName":"Amir Mohammad Kharazi","userId":"18414030253865360797"}},"outputId":"628feb58-2a73-479d-d4b8-5f78bc31e687"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (0.7.0)\n"]}]},{"cell_type":"markdown","source":["# Patch Embedding Class\n","\n","process :\n","\n","- input B C H W\n","- B C H W --> B (p1 p2) (h w c)\n","  - where h = H/patch_size , w = W/ patch_size\n","  - and p1, p2 = patch index\n","- output B N D"],"metadata":{"id":"n1SL3Gk7XuJM"}},{"cell_type":"code","source":["class PatchEmbedding(nn.Module):\n","    def __init__(self, img_size, patch_size, in_channels, embed_dim):\n","        super(PatchEmbedding, self).__init__()\n","        self.img_size = img_size\n","        self.patch_size = patch_size\n","        self.projection = nn.Linear(patch_size * patch_size * in_channels, embed_dim)\n","\n","    def forward(self, x):\n","        batch_size, channels, height, width = x.shape\n","        x = rearrange(x, 'b c (p1 h) (p2 w) -> b (p1 p2) (h w c)', h=self.patch_size, w=self.patch_size)\n","        x = self.projection(x) # x = [B, N, D]\n","        return x, x.shape[1] # patches and num of patches"],"metadata":{"id":"BgrxRh9uXxxP","executionInfo":{"status":"ok","timestamp":1703856867922,"user_tz":-210,"elapsed":36,"user":{"displayName":"Amir Mohammad Kharazi","userId":"18414030253865360797"}}},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":["# Multi Head Attention\n","\n","process :\n","\n","- input B N D\n","- q = linear projection of input to D' = B N D'\n","- k = linear projection of input to D' = B N D'\n","- v = linear projection of input to D' = B N D'\n","- Rearrange into number of heads (D' is divisible by h, number of heads):\n","  - Q = B N D'=(h d) ---> B h n d where h is the number of head and d is the head dimension\n","  - K = B N D'=(h d) ---> B h n d where h is the number of head and d is the head dimension\n","  - V = B N D'=(h d) ---> B h n d where h is the number of head and d is the head dimension\n","\n","- Q*K -> B h n n (multiplication on d and d of Q and K)\n","- scaled and softmaxed\n","- *V -> B h n d\n","-  Rearranged to B h n d --> B n (h d)\n","\n","\n","- in cvt Q*K -> B h n1 n2 where n1 could be equal to n2\n","  \n"],"metadata":{"id":"NEQjdyNbYMmr"}},{"cell_type":"code","source":["class MultiHeadAttention(nn.Module):\n","    def __init__(self, embed_dim, num_heads):\n","        super(MultiHeadAttention, self).__init__()\n","        self.embed_dim = embed_dim\n","        self.num_heads = num_heads\n","        self.head_dim = embed_dim // num_heads\n","        self.scale = self.head_dim ** -0.5\n","\n","        # first dim is the dimension of embedded x second dim is the embedded dimension of q/k/v (could be different)\n","        self.query = nn.Linear(embed_dim, embed_dim)\n","        self.key = nn.Linear(embed_dim, embed_dim)\n","        self.value = nn.Linear(embed_dim, embed_dim)\n","\n","        # fc_out as the final embedding of the attention. commonly used\n","        # self.fc_out = nn.Linear(embed_dim, embed_dim)\n","\n","    def forward(self, x, mask=None):\n","\n","        Q = rearrange(self.query(x), 'b n (h d) -> b h n d', h=self.num_heads)\n","        K = rearrange(self.key(x), 'b n (h d) -> b h n d', h=self.num_heads)\n","        V = rearrange(self.value(x), 'b n (h d) -> b h n d', h=self.num_heads)\n","\n","        # Attention calculation\n","        attn = torch.einsum('bhid,bhjd->bhij', Q, K) * self.scale\n","\n","        attention = F.softmax(attn, dim=-1)\n","        x = torch.einsum('bhij,bhjd->bhid', attention, V)\n","        x = rearrange(x, 'b h n d -> b n (h d)')\n","        # x = self.fc_out(x)\n","        return x"],"metadata":{"id":"VuHxZK_8YQTA","executionInfo":{"status":"ok","timestamp":1703856867923,"user_tz":-210,"elapsed":32,"user":{"displayName":"Amir Mohammad Kharazi","userId":"18414030253865360797"}}},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":["# Transformers Block (Encode Blocks)\n","\n","process :\n","\n","- define the attention block\n","- define the mlp block\n","  - two layers / GELU activation function\n","  - from embedded dimension (output of attention) to mlp dim  \n","  - and from mlp dim to embedded dimension (output of mlp usually the same as embed_dim)\n","- define layer norm"],"metadata":{"id":"1oBCzZcQYSux"}},{"cell_type":"code","source":["class TransformerBlock(nn.Module):\n","    def __init__(self, embed_dim, num_heads, mlp_dim, dropout=0.1):\n","        super(TransformerBlock, self).__init__()\n","        self.attention = MultiHeadAttention(embed_dim, num_heads)\n","        self.norm1 = nn.LayerNorm(embed_dim)\n","        self.mlp = nn.Sequential(\n","            nn.Linear(embed_dim, mlp_dim),\n","            nn.GELU(),\n","            nn.Linear(mlp_dim, embed_dim),\n","            nn.Dropout(dropout)\n","        )\n","        self.norm2 = nn.LayerNorm(embed_dim)\n","        self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self, x):\n","        x_res = x\n","        x = self.norm1(x + self.dropout(self.attention(x)))\n","        x = self.norm2(x + self.dropout(self.mlp(x)))\n","        return x + x_res"],"metadata":{"id":"lEObvQhTYZwQ","executionInfo":{"status":"ok","timestamp":1703856867924,"user_tz":-210,"elapsed":29,"user":{"displayName":"Amir Mohammad Kharazi","userId":"18414030253865360797"}}},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":["# Vision Transformer\n","\n","process :\n","\n","- img_size is the size of the image\n","- patch size is the size of the patches\n","- in_channels is the number of input image channels\n","- num_classes is the number of classes\n","- embed_dim is the embedding dimension (for simplification we consider it to be equal in many layers)\n","- num_heads is the number of heads\n","- num_layers is the number of encode/transformer blocks\n","- mlp_dim is the hidden dimension of the mlp block\n","- dropout is the drop out rate\n","\n","\n","pos embedding :\n","\n","- parameter added to the embedding patches\n","- input tensor is B (N) D , after cls token concatation\n","- pos embed shape is 1 N D\n","- simply add it to the input tensor / pytorch will broadcast it to all batches\n","\n","cls_token :\n","\n","- learnable parameters, a single learned embedding representing global information about the entire image.\n","- input tensor is B N D ,\n","- cls token parameter is 1 1 D\n","- expand cls token to (x.shape[0], -1, -1) : replicate for all batches\n","- cls token shape is B 1 D\n","- concat with input tensor on dim 1 (N - > N+1)\n","- result is B (N+1) D\n","\n","\n","\n","classification :\n","\n","- different methods\n","- google research simple vit uses cls token (learned) to classify images\n","\n"],"metadata":{"id":"UP-SxHfzYglO"}},{"cell_type":"code","execution_count":6,"metadata":{"id":"v2fnbI-4XCzN","executionInfo":{"status":"ok","timestamp":1703856867925,"user_tz":-210,"elapsed":29,"user":{"displayName":"Amir Mohammad Kharazi","userId":"18414030253865360797"}}},"outputs":[],"source":["class VisionTransformer(nn.Module):\n","    def __init__(self,\n","                 img_size=224,\n","                 patch_size=16,\n","                 in_channels=3,\n","                 num_classes=1000,\n","                 embed_dim=768,\n","                 num_heads=12,\n","                 num_layers=12,\n","                 mlp_dim=3072,\n","                 dropout=0.1\n","                 ):\n","        super(VisionTransformer, self).__init__()\n","\n","        self.patch_embedding = PatchEmbedding(img_size, patch_size, in_channels, embed_dim)\n","\n","        self.pos_embedding = nn.Parameter(torch.randn(1, (img_size // patch_size) ** 2, embed_dim))\n","\n","        self.cls_token = nn.Parameter(torch.randn(1, 1, embed_dim))\n","\n","        self.transformer = nn.ModuleList([\n","            TransformerBlock(embed_dim, num_heads, mlp_dim, dropout) for _ in range(num_layers)\n","        ])\n","        self.norm = nn.LayerNorm(embed_dim)\n","        # embed layer is the output of final mlp layer in the transformer block\n","        self.fc = nn.Linear(embed_dim, num_classes)\n","\n","    def forward(self, x):\n","        patches, num_patches = self.patch_embedding(x)\n","\n","        patches += self.pos_embedding\n","\n","        cls_tokens = self.cls_token.expand(x.shape[0], -1, -1)\n","        x = torch.cat([cls_tokens, patches], dim=1)\n","\n","        for transformer_block in self.transformer:\n","            x = transformer_block(x)\n","\n","        x = self.norm(x)\n","        cls_token_final = x[:, 0]\n","        output = self.fc(cls_token_final)\n","        return output\n"]},{"cell_type":"markdown","source":["## Example"],"metadata":{"id":"dp8c21p6Yj_S"}},{"cell_type":"code","source":["vit = VisionTransformer(img_size=224, patch_size=16, in_channels=3, num_classes=10)\n","\n","dummy_input = torch.randn(6, 3, 224, 224)\n","output = vit(dummy_input)\n","print(output.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nOIs6EyYYlhQ","executionInfo":{"status":"ok","timestamp":1703856918087,"user_tz":-210,"elapsed":4673,"user":{"displayName":"Amir Mohammad Kharazi","userId":"18414030253865360797"}},"outputId":"a348cf4c-6c60-4bae-c81c-378d878ea3a4"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([6, 10])\n"]}]}]}